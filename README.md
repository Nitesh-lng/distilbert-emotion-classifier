# ğŸ§  DistilBERT Emotion Classifier  

## ğŸš€ Overview  
This project uses **DistilBERT** to extract sentence embeddings from the **Hugging Face "emotion" dataset** and trains a **Logistic Regression classifier** to predict emotions.  

## ğŸ“Œ Features  
âœ… Loads and preprocesses the **emotion dataset**  
âœ… Tokenizes text using **DistilBERT tokenizer**  
âœ… Extracts **hidden state embeddings** from DistilBERT  
âœ… Trains a **Logistic Regression model** for classification  
âœ… Evaluates model performance on the validation set  

## ğŸ“‚ Dataset  
- The dataset comes from Hugging Face: [`emotion`](https://huggingface.co/datasets/emotion)  
- Contains **text samples** labeled with emotions like **joy, sadness, anger, etc.**  

## ğŸ› ï¸ Installation & Requirements  

### **1ï¸âƒ£ Install Dependencies**  

ğŸ¯ Project Workflow
1ï¸âƒ£ Load the Emotion Dataset
2ï¸âƒ£ Tokenize the text using DistilBERT
3ï¸âƒ£ Extract hidden states (sentence embeddings) from DistilBERT
4ï¸âƒ£ Store embeddings in the dataset
5ï¸âƒ£ Train a Logistic Regression model on embeddings
6ï¸âƒ£ Evaluate the model on validation data

ğŸš€ Model Performance
Model used: DistilBERT + Logistic Regression
Evaluation metric: Accuracy on validation set
ğŸ“Š Expected Results
With proper tuning, you can achieve ~90% accuracy on emotion classification.
Possible improvements:
Fine-tuning DistilBERT instead of using static embeddings.
Trying other classifiers like SVM or Neural Networks.
ğŸ¤– Future Enhancements
ğŸ”¹ Fine-tune DistilBERT on the emotion dataset
ğŸ”¹ Experiment with different ML classifiers
ğŸ”¹ Deploy as an API using FastAPI/Flask

